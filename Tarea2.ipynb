{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "volDPWJCEApo"
   },
   "source": [
    "# Introducción a la Ciencia de Datos: Tarea 2\n",
    "\n",
    "Este notebook contiene el código de base para realizar la Tarea 2 del curso. Puede copiarlo en su propio repositorio y trabajar sobre el mismo.\n",
    "Las **instrucciones para ejecutar el notebook** están en la [página inicial del repositorio](https://gitlab.fing.edu.uy/maestria-cdaa/intro-cd/).\n",
    "\n",
    "**Se espera que no sea necesario revisar el código para corregir la tarea**, ya que todos los resultados y análisis relevantes deberían estar en el **informe en formato PDF**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCfHPoNAEApr"
   },
   "source": [
    "## Cargar dependencias\n",
    "Para esta tarea, se han agregado algunos requerimientos, asegúrese de instalarlos (puede usar el mismo entorno virtual de la Tarea 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "daU7s9BMEApr",
    "outputId": "f76afe9e-0c5c-4899-e940-e1980fdc8e46"
   },
   "outputs": [],
   "source": [
    "# !pip install jupyter pandas \"sqlalchemy<2.0\" pymysql seaborn pillow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F51LwEKDEAps"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0BOzwdOEAps"
   },
   "source": [
    "## Conexión a la Base y Lectura de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JDVF3-pDEAps",
    "outputId": "95d291e2-70ad-4aaa-a64f-7524a2b3446e"
   },
   "outputs": [],
   "source": [
    "data_dir = Path(\"data\") / \"shakespeare\"\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def load_table(table_name, engine):\n",
    "    \"\"\"\n",
    "    Leer la tabla con SQL y guardarla como CSV,\n",
    "    o cargarla desde el CSV si ya existe\n",
    "    \"\"\"\n",
    "    path_table = data_dir / f\"{table_name}.csv\"\n",
    "    if not path_table.exists():\n",
    "        print(f\"Consultando tabla con SQL: {table_name}\")\n",
    "        t0 = time()\n",
    "        df_table = pd.read_sql(f\"SELECT * FROM {table_name}\", engine)\n",
    "        t1 = time()\n",
    "        print(f\"Tiempo: {t1 - t0:.1f} segundos\")\n",
    "\n",
    "        print(f\"Guardando: {path_table}\\n\")\n",
    "        df_table.to_csv(path_table)\n",
    "    else:\n",
    "        print(f\"Cargando tabla desde CSV: {path_table}\")\n",
    "        df_table = pd.read_csv(path_table, index_col=[0])\n",
    "    return df_table\n",
    "\n",
    "\n",
    "print(\"Conectando a la base...\")\n",
    "conn_str = \"mysql+pymysql://guest:relational@relational.fit.cvut.cz:3306/Shakespeare\"\n",
    "engine = create_engine(conn_str)\n",
    "\n",
    "# Todos los párrafos de todas las obras\n",
    "df_paragraphs = load_table(\"paragraphs\", engine)\n",
    "\n",
    "df_characters = load_table(\"characters\", engine)\n",
    "\n",
    "df_works = load_table(\"works\", engine)\n",
    "\n",
    "df_chapters = load_table(\"chapters\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "wd4J9X5CEApt",
    "outputId": "abf698e9-cbf5-4597-a4bc-768b3ca7a870"
   },
   "outputs": [],
   "source": [
    "df_paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc_qrFm0EApt"
   },
   "source": [
    "## Limpieza de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "vE0f9maqEApu",
    "outputId": "470a44ab-b163-4efe-a6b1-215aee9ee590",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: Actualizar con su versión de clean_text() de la Tarea_1\n",
    "\n",
    "\n",
    "def clean_text(df, column_name):\n",
    "    # Convertir todo a minúsculas\n",
    "    result = df[column_name].str.lower()\n",
    "\n",
    "    # Quitar signos de puntuación y cambiarlos por espacios (\" \")\n",
    "    # TODO: completar signos de puntuación faltantes\n",
    "    for punc in [\"[\", \"\\n\", \",\", \";\", \"]\", \".\", \":\", \"!\", \"¡\", \"?\", \"¿\", \"-\", \" '\", \"' \"]:\n",
    "        result = result.str.replace(punc, \" \")\n",
    "    return result\n",
    "\n",
    "# Creamos una nueva columna CleanText a partir de PlainText\n",
    "df_paragraphs[\"CleanText\"] = clean_text(df_paragraphs, \"PlainText\")\n",
    "\n",
    "\n",
    "def clean_text_stopwords (df, column_name):\n",
    "\n",
    "    result = df[column_name].str.lower()\n",
    "\n",
    "    for word in stops:\n",
    "        result = result.str.replace(\" \"+ word +\" \", \" \")\n",
    "    return result\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# stops = set(stopwords.words('english'))\n",
    "# df_paragraphs[\"CleanText\"] = clean_text_stopwords(df_paragraphs, \"CleanText\")\n",
    "\n",
    "# Veamos la diferencia\n",
    "df_paragraphs[[\"PlainText\", \"CleanText\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "ai_G-8nlEApu",
    "outputId": "b1bd991c-70c4-486b-f82c-817d8fb60a38"
   },
   "outputs": [],
   "source": [
    "# Agregamos personajes, obras y géneros en el mismo dataset\n",
    "df_dataset = df_paragraphs.merge(df_chapters.set_index(\"id\")[\"work_id\"], left_on=\"chapter_id\", right_index=True)\n",
    "df_dataset = df_dataset.merge(df_works.set_index(\"id\")[[\"Title\", \"GenreType\"]], left_on=\"work_id\", right_index=True)\n",
    "df_dataset = df_dataset.merge(df_characters.set_index('id')[\"CharName\"], left_on=\"character_id\", right_index=True).sort_index()\n",
    "df_dataset = df_dataset[[\"CleanText\", \"CharName\", \"Title\", \"GenreType\"]]\n",
    "\n",
    "# Usaremos sólo estos personajes\n",
    "characters = [\"Antony\", \"Cleopatra\", \"Queen Margaret\"]\n",
    "df_dataset = df_dataset[df_dataset[\"CharName\"].isin(characters)]\n",
    "\n",
    "df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2y4j9_zZEApu",
    "outputId": "610b6312-6c3f-4c7c-8a9b-5373431cc27f"
   },
   "outputs": [],
   "source": [
    "# Párrafos por cada personaje seleccionado\n",
    "df_dataset[\"CharName\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CE38h5GEApu"
   },
   "source": [
    "## Dataset y Features de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gye-3gcNEApv"
   },
   "outputs": [],
   "source": [
    "X = df_dataset[\"CleanText\"].to_numpy()\n",
    "y = df_dataset[\"CharName\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cK7-IWErEApv",
    "outputId": "62b5394a-b8ea-4168-ac46-33283b462f4e"
   },
   "outputs": [],
   "source": [
    "\n",
    "# -> Definir X_train, X_test, y_train, y_test\n",
    "\n",
    "# X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=345)\n",
    "\n",
    "print(f\"Tamaños de Train/Test: {len(X_train)}/{len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "S3i5CIAFEApv",
    "outputId": "2c73ad3d-5c3e-4ad5-ead2-a21d7a3ec0f3"
   },
   "outputs": [],
   "source": [
    "Cleo_train=sum(1 for item in y_train if item==(\"Cleopatra\"))\n",
    "Anto_train=sum(1 for item in y_train if item==(\"Antony\"))\n",
    "Que_train=sum(1 for item in y_train if item==(\"Queen Margaret\"))\n",
    "\n",
    "Cleo_test=sum(1 for item in y_test if item==(\"Cleopatra\"))\n",
    "Anto_test=sum(1 for item in y_test if item==(\"Antony\"))\n",
    "Que_test=sum(1 for item in y_test if item==(\"Queen Margaret\"))\n",
    "\n",
    "nombres_cat = [\"Cleopatra\",\"Antony\",\"Queen Margaret\"]\n",
    "plt.bar(nombres_cat,[Cleo_train, Anto_train, Que_train], color = '#F29727',label=\"Train\",width=0.5)\n",
    "plt.bar(nombres_cat,[Cleo_test, Anto_test, Que_test], color = '#1B6B93', bottom=[Cleo_train, Anto_train, Que_train],label=\"Test\",width=0.5)\n",
    "plt.xlabel(\"Personajes\")\n",
    "plt.ylabel(\"Cantidad de parrafos\")\n",
    "plt.title('Proporcion de parrafos por personaje en los conjuntos de datos')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# nombres_cat = [\"Train\",\"Test\"]\n",
    "# plt.bar(nombres_cat,[Cleo_train, Cleo_test], color = '#F29727',label=\"Cleopatra\",width=0.35)\n",
    "# plt.bar(nombres_cat,[Anto_train, Anto_test], color = '#1B6B93', bottom=[Cleo_train, Cleo_test],label=\"Antony\",width=0.35)\n",
    "# plt.bar(nombres_cat,[Que_train, Que_test], color = '#88DC65', bottom=[Anto_train+Cleo_train, Anto_test+Cleo_test],label=\"Queen Margaret\",width=0.35)\n",
    "# plt.xlabel(\"Personajes\")\n",
    "# plt.ylabel(\"Cantidad de parrafos\")\n",
    "# plt.title('Proporcion de parrafos por personaje en los conjuntos de datos')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# categories = [\" \",\"Antony\",\"Queen Margaret\", \"Cleopatra\"]\n",
    "\n",
    "# radar_train = [Cleo_train, Anto_train, Que_train,Cleo_train]\n",
    "# radar_test = [Cleo_test, Anto_test, Que_test,Cleo_test]\n",
    "\n",
    "# #label_loc = np.linspace(start=0, stop=2 * np.pi, num=len(radar_train))\n",
    "# label_loc = np.array([np.pi/2, np.pi/2 + 2*np.pi/3, np.pi/2 + 4*np.pi/3, np.pi/2 ])\n",
    "\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.subplot(polar=True)\n",
    "# plt.plot(label_loc, radar_train, label='Train')\n",
    "# plt.plot(label_loc, radar_test, label='Test')\n",
    "\n",
    "# plt.title(' ', size=20)\n",
    "# lines, labels = plt.thetagrids(np.degrees(label_loc), labels=categories)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqqGi2DZEApv"
   },
   "source": [
    "### Conteo de palabras y TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cy2BEoHzEApv",
    "outputId": "e81305eb-8f7c-42d6-d701-a394062c1329"
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words=None, ngram_range=(1,2))\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts\n",
    "  ##TODO:\n",
    "    ##Explicar en el documento lastecnicas planteadas en la tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vKWdJ19kEApw",
    "outputId": "d9d816b2-db6c-4775-f91a-43477ad7bc60"
   },
   "outputs": [],
   "source": [
    "tf_idf = TfidfTransformer(use_idf=True)\n",
    "X_train_tf = tf_idf.fit_transform(X_train_counts)\n",
    "X_train_tf\n",
    "\n",
    "  ##TODO:\n",
    "    ##Explicar en el documento lastecnicas planteadas en la tarea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VdingaaEApw"
   },
   "source": [
    "### Reducción de dimensionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tpmGaFaVEApw"
   },
   "outputs": [],
   "source": [
    "# TODO: Realizar PCA sobre los datos de entrenamiento\n",
    "\n",
    "reductor = PCA(n_components=10)\n",
    "\n",
    "# Transformar train\n",
    "X_train_red = reductor.fit_transform(X_train_tf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "YagLKZ5fEApw",
    "outputId": "b6c318b3-92f1-4d10-b8b3-3b48a5f87e99"
   },
   "outputs": [],
   "source": [
    "var_ratio = reductor.explained_variance_ratio_\n",
    "acumulacion_varianza= np.cumsum(var_ratio)\n",
    "\n",
    "plt.bar(range(1,len(var_ratio)+1), var_ratio, alpha=0.5, align='center', label='Varianza explicada individual')\n",
    "plt.step(range(1,len(acumulacion_varianza)+1), acumulacion_varianza, where='mid',label='Acumulacion de varianza explicada')\n",
    "plt.xlabel(\"Numero de componetes\")\n",
    "plt.ylabel(\"Ralacion de la varianza explicada de los datos\")\n",
    "plt.legend(loc='best')\n",
    "plt.xticks(range(1,len(var_ratio)+1))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "id": "_xrwU3XLEApw",
    "outputId": "f095e786-4239-491e-a83d-36e6a0965a36"
   },
   "outputs": [],
   "source": [
    "# Visualización de las dos primeras componentes de PCA\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "for character in np.unique(y_train):\n",
    "    mask_train = y_train == character\n",
    "    ax.scatter(X_train_red[mask_train, 0], X_train_red[mask_train, 1], label=character)\n",
    "\n",
    "ax.set_title(\"PCA por personaje\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3fiRzi7EApw"
   },
   "source": [
    "## Modelos de Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "muBv2UpIEApx",
    "outputId": "0d0f8130-d12b-49bc-cbd4-dca8bb93b517",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bayes_clf = MultinomialNB().fit(X_train_tf, y_train)\n",
    "\n",
    "\n",
    "# Ver las primeras 10 predicciones de train\n",
    "y_pred_train = bayes_clf.predict(X_train_tf)\n",
    "y_pred_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "cupa8cyXEApx",
    "outputId": "3e4a0eff-10e6-46df-d3ce-0b9d6a649abb"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_pred):\n",
    "    return (y_true == y_pred).sum() / len(y_true)\n",
    "\n",
    "get_accuracy(y_train, y_pred_train)\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_train, y_pred_train )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "hjosoqncEApx",
    "outputId": "088d04eb-7c6a-4c81-e7d5-8ac910242a50",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_test_counts = count_vect.transform(X_test)\n",
    "X_test_tf = tf_idf.transform(X_test_counts)\n",
    "y_pred_test = bayes_clf.predict(X_test_tf)\n",
    "\n",
    "def get_accuracy(y_true, y_pred):\n",
    "    return (y_true == y_pred).sum() / len(y_true)\n",
    "\n",
    "get_accuracy(y_test, y_pred_test)\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_test )\n",
    "plt.show()\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred_test, target_names=[\"Antony\",\"Cleopatra\",\"Queen Margaret\"]))\n",
    "\n",
    "## TODO:\n",
    "\n",
    "   ##  Explique cómo se relacionan estos valores con la matriz anterior.\n",
    "   ## ¿Qué problemas puede tener el hecho de mirar sólamente el valor de accuracy?\n",
    "   ## Considere qué sucedería con esta métrica si el desbalance de datos fuera aún mayor entre personajes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IekhaHMaEApx"
   },
   "source": [
    "### Búsqueda de hiper-parámetros con Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EkUuL0VsEApx",
    "outputId": "2ab9aa4d-efd3-472b-c7b2-7980ad4235ea",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# TODO: Agregar más variantes de parámetros que les parezcan relevantes\n",
    "param_sets = [{\"stop_words\": None, \"ngram\": (1,2), \"idf\": True, \"alpha\":1.0},\n",
    "             {\"stop_words\": None, \"ngram\": (1,1), \"idf\": False, \"alpha\":1.0},\n",
    "             {\"stop_words\": 'english', \"ngram\": (1,1), \"idf\": False, \"alpha\":1.0},\n",
    "             {\"stop_words\": 'english', \"ngram\": (1,1), \"idf\": True, \"alpha\":1.0},\n",
    "             {\"stop_words\": 'english', \"ngram\": (1,2), \"idf\": False, \"alpha\":1.0},\n",
    "             {\"stop_words\": 'english', \"ngram\": (1,1), \"idf\": False, \"alpha\":2.0},\n",
    "             {\"stop_words\": 'english', \"ngram\": (1,1), \"idf\": False, \"alpha\":0.5},\n",
    "             {\"stop_words\": 'english', \"ngram\": (1,1), \"idf\": False, \"alpha\":0.4},\n",
    "             {\"stop_words\": 'english', \"ngram\": (1,1), \"idf\": False, \"alpha\":0.35}]\n",
    "ns=5\n",
    "skf = StratifiedKFold(n_splits=ns, shuffle=True, random_state=42)\n",
    "\n",
    "# Ahora usaremos train/validation/test\n",
    "# Por lo tanto le renombramos train+validation = dev(elopment) dataset\n",
    "X_dev = X_train\n",
    "y_dev = y_train\n",
    "\n",
    "# # Para evitar errores\n",
    "#del X_train\n",
    "#del y_train\n",
    "M=np.zeros((ns,len(param_sets)))\n",
    "for i, params in enumerate(param_sets):\n",
    "\n",
    "    # Transormaciones a aplicar (featurizers)\n",
    "    count_vect = CountVectorizer(stop_words=params[\"stop_words\"], ngram_range=params[\"ngram\"])\n",
    "    tf_idf = TfidfTransformer(use_idf=params[\"idf\"])\n",
    "    j=0\n",
    "    for train_idxs, val_idxs in skf.split(X_dev, y_dev):\n",
    "\n",
    "        # Train y validation para el split actual\n",
    "        X_train_ = X_dev[train_idxs]\n",
    "        y_train_ = y_dev[train_idxs]\n",
    "        X_val = X_dev[val_idxs]\n",
    "        y_val = y_dev[val_idxs]\n",
    "\n",
    "        # Ajustamos y transformamos Train\n",
    "        X_train_counts = count_vect.fit_transform(X_train_)\n",
    "        X_train_tf = tf_idf.fit_transform(X_train_counts)\n",
    "\n",
    "        # TODO: Completar el código para entrenar y evaluar\n",
    "\n",
    "        # Entrenamos con Train\n",
    "        bayes_clf = MultinomialNB(alpha=params[\"alpha\"]).fit(X_train_tf, y_train_)\n",
    "\n",
    "        # Transformamos Validation\n",
    "        X_val_counts = count_vect.transform(X_val)\n",
    "        X_val_tfidf = tf_idf.transform(X_val_counts)\n",
    "\n",
    "\n",
    "        # Predecimos y evaluamos en Validation\n",
    "        y_pred_val = bayes_clf.predict(X_val_tfidf)\n",
    "        acc = get_accuracy(y_val, y_pred_val)\n",
    "        print(f\"{acc=:.4f} {params=}\")\n",
    "        M[j,i]=acc\n",
    "        j=j+1\n",
    "\n",
    "plt.figure()\n",
    "plt.violinplot(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "id": "KdcKiCjXEApx",
    "outputId": "55e2c96a-a21c-460c-ff1f-95af134ca576"
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words='english', ngram_range=(1,1))\n",
    "tf_idf = TfidfTransformer(use_idf=False)\n",
    "\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_tf = tf_idf.fit_transform(X_train_counts)\n",
    "\n",
    "bayes_clf = MultinomialNB(alpha=0.4).fit(X_train_tf, y_train)\n",
    "\n",
    "X_val_counts = count_vect.transform(X_test)\n",
    "X_val_tfidf = tf_idf.transform(X_val_counts)\n",
    "\n",
    "y_pred_test = bayes_clf.predict(X_val_tfidf)\n",
    "\n",
    "def get_accuracy(y_true, y_pred):\n",
    "    return (y_true == y_pred).sum() / len(y_true)\n",
    "\n",
    "get_accuracy(y_test, y_pred_test)\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_test )\n",
    "plt.show()\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred_test, target_names=[\"Antony\",\"Cleopatra\",\"Queen Margaret\"]))\n",
    "\n",
    "##TODO\n",
    "    #Discuta las limitaciones de utilizar un modelo basado en bag-of-words o tf-idf en cuanto al análisis de texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "id": "OYfitmPcEApy",
    "outputId": "ca5a07e3-81a2-4f6d-935d-1b71dac2e45f"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "\n",
    "SGDC_clf = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, max_iter=1000, tol=1).fit(X_train_tf, y_train)\n",
    "\n",
    "y_pred_test = SGDC_clf.predict(X_val_tfidf)\n",
    "\n",
    "def get_accuracy(y_true, y_pred):\n",
    "    return (y_true == y_pred).sum() / len(y_true)\n",
    "\n",
    "get_accuracy(y_test, y_pred_test)\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_test )\n",
    "plt.show()\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred_test, target_names=[\"Antony\",\"Cleopatra\",\"Queen Margaret\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQ3nl_bYEApy"
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# parameters = {'vect__ngram_range': [(1, 1)],'tfidf__use_idf': (True),'clf__alpha': (1e-2)}\n",
    "\n",
    "# gs_clf = GridSearchCV(estimator=SVC(), param_grid=parameters, cv=5, n_jobs=-1)\n",
    "# gs_clf = gs_clf.fit(X_train_tf, y_train)\n",
    "\n",
    "# y_pred_test = gs_clf.predict(X_val_tfidf)\n",
    "\n",
    "# def get_accuracy(y_true, y_pred):\n",
    "#     return (y_true == y_pred).sum() / len(y_true)\n",
    "\n",
    "# get_accuracy(y_test, y_pred_test)\n",
    "# ConfusionMatrixDisplay.from_predictions(y_test, y_pred_test )\n",
    "# plt.show()\n",
    "\n",
    "# from sklearn import metrics\n",
    "\n",
    "# print(metrics.classification_report(y_test, y_pred_test, target_names=[\"Antony\",\"Cleopatra\",\"Queen Margaret\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OQCgUCYSEApy",
    "outputId": "717bd98a-2838-4cbe-b4ec-05d3c1433e1a"
   },
   "outputs": [],
   "source": [
    "# Agregamos personajes, obras y géneros en el mismo dataset\n",
    "df_dataset = df_paragraphs.merge(df_chapters.set_index(\"id\")[\"work_id\"], left_on=\"chapter_id\", right_index=True)\n",
    "df_dataset = df_dataset.merge(df_works.set_index(\"id\")[[\"Title\", \"GenreType\"]], left_on=\"work_id\", right_index=True)\n",
    "df_dataset = df_dataset.merge(df_characters.set_index('id')[\"CharName\"], left_on=\"character_id\", right_index=True).sort_index()\n",
    "df_dataset = df_dataset[[\"CleanText\", \"CharName\", \"Title\", \"GenreType\"]]\n",
    "\n",
    "# Usaremos sólo estos personajes\n",
    "characters = [\"Falstaff\", \"Cleopatra\", \"Queen Margaret\"]\n",
    "df_dataset = df_dataset[df_dataset[\"CharName\"].isin(characters)]\n",
    "\n",
    "df_dataset\n",
    "\n",
    "# Párrafos por cada personaje seleccionado\n",
    "df_dataset[\"CharName\"].value_counts()\n",
    "\n",
    "\n",
    "X = df_dataset[\"CleanText\"].to_numpy()\n",
    "y = df_dataset[\"CharName\"].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=345)\n",
    "\n",
    "print(f\"Tamaños de Train/Test: {len(X_train)}/{len(X_test)}\")\n",
    "\n",
    "Cleo_train=sum(1 for item in y_train if item==(\"Cleopatra\"))\n",
    "Fal_train=sum(1 for item in y_train if item==(\"Falstaff\"))\n",
    "Que_train=sum(1 for item in y_train if item==(\"Queen Margaret\"))\n",
    "\n",
    "Cleo_test=sum(1 for item in y_test if item==(\"Cleopatra\"))\n",
    "Fal_test=sum(1 for item in y_test if item==(\"Falstaff\"))\n",
    "Que_test=sum(1 for item in y_test if item==(\"Queen Margaret\"))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "nombres_cat = [\"Cleopatra\",\"Falstaff\",\"Queen Margaret\"]\n",
    "plt.bar(nombres_cat,[Cleo_train, Fal_train, Que_train], color = '#F29727',label=\"Train\",width=0.5)\n",
    "plt.bar(nombres_cat,[Cleo_test, Fal_test, Que_test], color = '#1B6B93', bottom=[Cleo_train, Fal_train, Que_train],label=\"Test\",width=0.5)\n",
    "plt.xlabel(\"Personajes\")\n",
    "plt.ylabel(\"Cantidad de parrafos\")\n",
    "plt.title('Proporcion de parrafos por personaje en los conjuntos de datos')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "X_train_tf = tf_idf.fit_transform(X_train_counts)\n",
    "\n",
    "reductor = PCA(n_components=10)\n",
    "X_train_red = reductor.fit_transform(X_train_tf.toarray())\n",
    "var_ratio = reductor.explained_variance_ratio_\n",
    "acumulacion_varianza= np.cumsum(var_ratio)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(range(1,len(var_ratio)+1), var_ratio, alpha=0.5, align='center', label='Varianza explicada individual')\n",
    "plt.step(range(1,len(acumulacion_varianza)+1), acumulacion_varianza, where='mid',label='Acumulacion de varianza explicada')\n",
    "plt.xlabel(\"Numero de componetes\")\n",
    "plt.ylabel(\"Relacion de la varianza explicada de los datos\")\n",
    "plt.legend(loc='best')\n",
    "plt.xticks(range(1,len(var_ratio)+1))\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "for character in np.unique(y_train):\n",
    "    mask_train = y_train == character\n",
    "    ax.scatter(X_train_red[mask_train, 0], X_train_red[mask_train, 1], label=character)\n",
    "ax.legend()\n",
    "ax.set_title(\"PCA por personaje\")\n",
    "\n",
    "\n",
    "bayes_clf = MultinomialNB(alpha=0.4).fit(X_train_tf, y_train)\n",
    "X_test_counts = count_vect.transform(X_test)\n",
    "X_test_tf = tf_idf.transform(X_test_counts)\n",
    "y_pred_test = bayes_clf.predict(X_test_tf)\n",
    "\n",
    "def get_accuracy(y_true, y_pred):\n",
    "    return (y_true == y_pred).sum() / len(y_true)\n",
    "\n",
    "get_accuracy(y_test, y_pred_test)\n",
    "plt.figure()\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_test )\n",
    "plt.show()\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred_test, target_names=[\"Cleopatra\",\"Falstaff\",\"Queen Margaret\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IP8LknvEApy"
   },
   "source": [
    "### (Opcional) Comparativa con Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pwST826cEApy",
    "outputId": "a2394b12-feab-4e20-e03c-7f03f0943260",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jkDY_tkzZd1Q"
   },
   "outputs": [],
   "source": [
    "# Cargamos nuevamente los datos\n",
    "df_dataset = df_paragraphs.merge(df_chapters.set_index(\"id\")[\"work_id\"], left_on=\"chapter_id\", right_index=True)\n",
    "df_dataset = df_dataset.merge(df_works.set_index(\"id\")[[\"Title\", \"GenreType\"]], left_on=\"work_id\", right_index=True)\n",
    "df_dataset = df_dataset.merge(df_characters.set_index('id')[\"CharName\"], left_on=\"character_id\", right_index=True).sort_index()\n",
    "df_dataset = df_dataset[[\"CleanText\", \"CharName\", \"Title\", \"GenreType\"]]\n",
    "\n",
    "# Usaremos sólo estos personajes\n",
    "characters = [\"Antony\", \"Cleopatra\", \"Queen Margaret\"]\n",
    "df_dataset = df_dataset[df_dataset[\"CharName\"].isin(characters)]\n",
    "\n",
    "df_dataset\n",
    "\n",
    "# Párrafos por cada personaje seleccionado\n",
    "df_dataset[\"CharName\"].value_counts()\n",
    "\n",
    "\n",
    "X = df_dataset[\"CleanText\"].to_numpy()\n",
    "y = df_dataset[\"CharName\"].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=345)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "kCajPU7pEApz",
    "outputId": "c6d2de78-a543-4e50-a7f0-331059267314"
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "y_train_s = np.char.replace(y_train.astype(str), \" \", \"_\").astype(object)\n",
    "y_test_s = np.char.replace(y_test.astype(str), \" \", \"_\").astype(object)\n",
    "\n",
    "# Convertimos al formato de fasttext: archivo de texto donde cada línea es:\n",
    "# __label__<label> TEXTO\n",
    "Xytrains = \"__label__\" + y_train_s.astype(object) + \" \" + X_train\n",
    "Xytests = \"__label__\" + y_test_s.astype(object) + \" \" + X_test\n",
    "np.savetxt(data_dir / \"train.txt\", Xytrains, fmt=\"%s\")\n",
    "np.savetxt(data_dir / \"test.txt\", Xytests, fmt=\"%s\")\n",
    "\n",
    "Xytests[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3dEw3gkEApz",
    "outputId": "cac24050-e373-41dc-b53b-420c49c1b028"
   },
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input=str(data_dir / \"train.txt\"), epoch=100, wordNgrams=2)\n",
    "model.test(str(data_dir / \"test.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSlzUnHLEApz",
    "outputId": "182e6842-6734-4fe6-faa2-a70f9e7da265"
   },
   "outputs": [],
   "source": [
    "y_out = model.predict(list(X_test))\n",
    "y_pred_test = [y[0].replace(\"__label__\", \"\") for y in y_out[0]]\n",
    "\n",
    "print(get_accuracy(y_test_s, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "W2E_BsV5EApz",
    "outputId": "de44c707-9176-4b59-806a-67b83846b1fb"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ConfusionMatrixDisplay.from_predictions(y_test_s, y_pred_test )\n",
    "plt.show()\n",
    "\n",
    "print(metrics.classification_report(y_test_s, y_pred_test, target_names=[\"Antony\",\"Cleopatra\",\"Queen_Margaret\"]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
